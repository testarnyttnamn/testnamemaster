{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO: CLOE\n",
    "\n",
    "**Description**: this DEMO allows to compute the Galaxy Clustering and Weak Lensing observational probes as defined in the current recipe and computes the likelihood value given some benchmark data. It uses `Cobaya` as the main Bayesian Analysis tool.\n",
    "\n",
    "It contains three potential different scenarios for the user (called user-cases) corresponding to CASE 1, 2 and 3 in the notebook. A basic description of each of the user-case is the following:\n",
    "\n",
    "* **CASE 1**: samples the posterior distribution of the parameters of interest (cosmological or nuisance parameters) by running `Cobaya` with the external Euclid Likelihood and the sampler of your choice within the list of samplers available in `Cobaya`. At the moment, this user case executes a single point evalution of the likelihood using the `Cobaya` sampler `evaluate`.\n",
    "\n",
    "* **CASE 2**: creates a `model` of `Cobaya` using an internal wrapper of `Cobaya` itself. This wrapper is based on the sampler `evaluate` of `Cobaya`. The `model` allows you to make a single computation of the priors, likelihoods and posterior, measure the time needed by each module or retrieve the information of which theoretical quantities were asked to the Boltzman solvers, for instance. This `model` instance is important, because it is the way the connection between `Cobaya` and the `EuclidLikelihood` code is made internally within `cobaya_interface.py`. In fact, the `model` instance is essential if you want to run CASE 2.1.\n",
    "\n",
    "* **CASE 2.1**: the final section of the notebook retrieves and plots internal theoretical quantities computed by the CLOE code. Be aware that, to be able to retrieve and plot quantities, you need to run first **CASE 2**, because CASE 2.1 requires the `model` instance to be loaded. You can plot the following quantities:\n",
    "    * $H(z)$: Huble factor\n",
    "    * $r(z)$, $D_A$: comoving and angular diameter distances\n",
    "    * $\\sigma_8$, $f\\sigma_8$: amplitude of the (linear) power spectrum on the scale of 8 $h^{-1} Mpc$ and the product of the growth rate and the amplitude of the (linear) power spectrum on the scale of 8 $h^{-1} Mpc$\n",
    "    * $D(z)$, $f(z)$: growth factor and growth rate\n",
    "    * $P_m, P_{gg}, P_{g\\delta}, P_{\\delta i}, P_{gi}, P_{ii}$: matter power spectrum obtained by the Boltzman solver by `Cobaya` and different power spectra\n",
    "    * $n(z)$: galaxy density distributions\n",
    "    * $W_i^{GC}, W_i^{\\gamma}, W_i^{IA}$: window functions or kernel for Galaxy Clustering (GC), Shear ($\\gamma$) and Intrinsic Aligment.\n",
    "    * $C_\\ell$: angular power spectra\n",
    "    * $P_\\ell$: Legendre multiples\n",
    "\n",
    "\n",
    "**README**: https://gitlab.euclid-sgs.uk/pf-ist-likelihood/likelihood-implementation/-/blob/master/README.md\n",
    "\n",
    "**Install**: in order to use this DEMO notebook, you need to clone the repository `https://gitlab.euclid-sgs.uk/pf-ist-likelihood/likelihood-implementation.git`, and install the CLOE as described in the README.\n",
    "Alternatively you may be ready to run if ```Cobaya``` and ```CAMB/CLASS``` are installed. See details below. \n",
    "\n",
    "**Cobaya documentation**: https://cobaya.readthedocs.io/en/latest/\n",
    "\n",
    "**Python information**: if the user is not confortable with some of the python vocabulary used in this notebook,\n",
    "a nice python review can be found here\n",
    "https://wiki.python.org/moin/BeginnersGuide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General python imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting as working directory:  /data2/EUCLID/likelihood-implementation\n"
     ]
    }
   ],
   "source": [
    "# If you have clone the repository and open this notebook,\n",
    "# this notebook should be in likelihood-implementation/notebooks\n",
    "# Let's set the working directory to be likelihood-implementation\n",
    "\n",
    "likelihood_path = os.path.realpath(os.path.join(os.getcwd(),'..'))\n",
    "sys.path.insert(0, likelihood_path)\n",
    "print('Setting as working directory: ', likelihood_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib params set-up\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "plt.rc('font',size=25)\n",
    "plt.rc('axes', titlesize=26)\n",
    "plt.rc('axes', labelsize=25)\n",
    "plt.rc('lines', linewidth=2)\n",
    "plt.rc('lines', markersize=6)\n",
    "plt.rc('legend', fontsize=20)\n",
    "plt.rc('mathtext', fontset='stix')\n",
    "plt.rc('font', family='STIXGeneral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cobaya` needs some modules to run: i.e: CAMB, CLASS, Polychord, other likelihood codes and data (Planck 18, DES...)\n",
    "If you have installed the CLOE as in the README instructions, you need to activate the conda environment `likelihood` to run. This conda environment has CAMB installed so you won't need to worry about anything else\n",
    "\n",
    "Alternatively, if you have already installed Cobaya and other Cosmological codes such as CAMB, CLASS, Polychord or Planck 18 you have 2 options:\n",
    "\n",
    "* **(1)**:  point out where each of them is installed with the flag 'path' in the dictionary of cell 5. See the comment in the cell 5 corresponding to the 'theory' key of the 'info' dictionary\n",
    "\n",
    "\n",
    "* **(2)**: if you installed the Cosmological codes as Cobaya automatic installation suggests  (https://cobaya.readthedocs.io/en/latest/installation_cosmo.html) you need to point out the path to your modules\n",
    "as in the variable `modules_path` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: write down the path to your COBAYA modules if you want to follow option (2) above.\n",
    "# Otherwise skip this cell\n",
    "# modules_path = \"/data2/cobaya_modules/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE 1: 'Run `Cobaya` with Euclid-Likelihood \n",
    "**User-case**: *Run and go*. This is the most straightforward case where the user calls `Cobaya` to sample the posterior distribution of the parameters of interest. At the moment, this user-case runs one computation of the likelihood on one point of the parameters space given some theoretical predictions of Euclid observables.\n",
    "\n",
    "To run, Cobaya needs an 'input file'. Please, read carefully the comments in the cells below to understand \n",
    "how this input file looks like and which options are available to be modified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external loglike from the Likelihood Package within cobaya_interface.py\n",
    "\n",
    "from likelihood.cobaya_interface import EuclidLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the yaml file corresponding to the Likelihood Parameters\n",
    "Cobaya needs to know which parameters the likelihood `EuclidLikelihood` expects (for example, nuisance parameters), apart from the cosmological parameters (i.e: $\\Omega_b$, $H_0$, $n_s$...) which are already known by the theory code (CAMB/CLASS). \n",
    "\n",
    "For that, a `yaml` file is generated instead of having those likelihood parameters hard-coded within `cobaya_interface.py`. To generate this `yaml` file, execute the two lines below.\n",
    "\n",
    "Please, be aware that this function will only make Cobaya expect those likelihood parameters. To set some fixed values or put a prior on them, the user needs to do so within the `info` python dictionary below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from likelihood.auxiliary.likelihood_params_yaml_generator import generate_params_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:\n",
      "\n",
      "Be aware that /data2/EUCLID/likelihood-implementation/likelihood/params.yaml has been overwritten\n",
      "/data2/EUCLID/likelihood-implementation/likelihood/params.yaml written\n"
     ]
    }
   ],
   "source": [
    "generate_params_yaml(model = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are running the Euclid-Likelihood as an external likelihood class for Cobaya\n",
    "# Cobaya needs a dictionary or yaml file as input to start running\n",
    "# This dictionary below ('info') can be modified up to some point by the user to\n",
    "# adapt it to the user's needs.\n",
    "# The options that can be modified by the user are pointed with the acronym (UC).\n",
    "\n",
    "info = {\n",
    "    #'params': Cobaya's protected key of the input dictionary. \n",
    "    # Includes the parameters that the user would like to sample over:\n",
    "'params': {\n",
    "        # (UC): each parameter below (which is a 'key' of another sub-dictionary) can contain a dictionary\n",
    "        # with the key 'prior', 'latex'...\n",
    "        # If the prior dictionary is not passed to a parameter, this parameter is fixed.\n",
    "        # In this example, we are sampling the parameter ns\n",
    "        # For more information see: https://cobaya.readthedocs.io/en/latest/example.html\n",
    "        'ombh2': 0.022445, #Omega density of baryons times the reduced Hubble parameter squared\n",
    "        'omch2': 0.1205579307, #Omega density of cold dark matter times the reduced Hubble parameter squared\n",
    "        'H0': 67, #Hubble parameter evaluated today (z=0) in km/s/Mpc\n",
    "        'tau': 0.0925, #optical depth\n",
    "        'mnu': 0.06, #  sum of the mass of neutrinos in eV\n",
    "        'nnu': 3.046, #N_eff of relativistic species \n",
    "        'As': 2.12605e-9, #Amplitude of the primordial scalar power spectrum\n",
    "        'ns': {'prior':{'min':0.8, 'max':1.2}}, # primordial power spectrum tilt (sampled with an uniform prior)\n",
    "        'w': -1, #Dark energy fluid model\n",
    "        'wa': 0, #Dark energy fluid model\n",
    "        'omk': 0.0, #curvature density\n",
    "        'omegam': None, #DERIVED parameter: Omega matter density\n",
    "        'omegab': None, #DERIVED parameter: Omega barion density\n",
    "        'omeganu': None, #DERIVED parameter: Omega neutrino density\n",
    "        'omnuh2': None, #DERIVED parameter: Omega neutrino density times de reduced Hubble parameter squared\n",
    "        'omegac': None, #DERIVED parameter: Omega cold dark matter density\n",
    "        'N_eff': None,\n",
    "        # (UC): change 'like_selection' based on which observational probe you would like to use. \n",
    "        # Choose among:\n",
    "        # 1: photometric survey\n",
    "        # 2: spectroscopic survey\n",
    "        # 12: both surveys\n",
    "        'like_selection': 12,\n",
    "        # (UC): if you selected the photometric survey (1) or both (12) in 'like_selection'\n",
    "        # you may want to choose between:\n",
    "        # using Galaxy Clustering photometric and Weak Lensing probes combined assuming they are independent ('full_photo': False)\n",
    "        # or Galaxy Clustering photometric, Weak Lensing and the cross-correlation between them ('full_photo': True)\n",
    "        # This flag is not used if 'like_selection: 2'\n",
    "        'full_photo': False,\n",
    "        # (UC): galaxy bias parameters:\n",
    "        # The bias parameters below are currently fixed to the\n",
    "        # values used by the Inter Science Taskforce: Forcast (IST:F)\n",
    "        # and presented in the corresponding IST:F paper (arXiv: 1910.09273).\n",
    "        # However, they can be changed by the user and even sample over them by putting a prior\n",
    "        # Photometric bias parameters\n",
    "        'b1_photo': 1.0997727037892875,\n",
    "        'b2_photo': 1.220245876862528,\n",
    "        'b3_photo': 1.2723993083933989,\n",
    "        'b4_photo': 1.316624471897739,\n",
    "        'b5_photo': 1.35812370570578,\n",
    "        'b6_photo': 1.3998214171814918,\n",
    "        'b7_photo': 1.4446452851824907,\n",
    "        'b8_photo': 1.4964959071110084,\n",
    "        'b9_photo': 1.5652475842498528,\n",
    "        'b10_photo': 1.7429859437184225,\n",
    "        # Spectroscopic bias parameters\n",
    "        'b1_spec': 1.46,\n",
    "        'b2_spec': 1.61,\n",
    "        'b3_spec': 1.75,\n",
    "        'b4_spec': 1.90,\n",
    "        'NL_flag': False,\n",
    "        'aia': 1.72,\n",
    "        'nia': -0.41,\n",
    "        'bia': 0.0},\n",
    "    #'theory': Cobaya's protected key of the input dictionary.\n",
    "    # Cobaya needs to ask some minimum theoretical requirements to a Boltzman Solver\n",
    "    # (UC): you can choose between CAMB or CLASS\n",
    "    # In this DEMO, we use CAMB and specify some CAMB arguments\n",
    "    # such as the number of massive neutrinos\n",
    "    # and the dark energy model\n",
    "    #\n",
    "    # ATTENTION: If you have CAMB/CLASS already installed and \n",
    "    # you are not using the likelihood conda environment \n",
    "    # or option (2) in cell (3) (Cobaya modules), you can add an extra key called 'path' within the camb dictionary\n",
    "    # to point to your already installed CAMB code\n",
    "    'theory': {'camb': \n",
    "               {'stop_at_error': True, \n",
    "                'extra_args':{'num_massive_neutrinos': 1,\n",
    "                              'dark_energy_model': 'ppf'}}},\n",
    "    #'sampler': Cobaya's protected key of the input dictionary.\n",
    "    # (UC): you can choose the sampler you want to use.\n",
    "    # Check Cobaya's documentation to see the list of available samplers\n",
    "    # In this DEMO, we use the 'evaluate' sampler to make a single computation of the posterior distributions\n",
    "    # Note: if you want to run a simple MCMC sampling choose 'mcmc'\n",
    "    'sampler': {'evaluate': None},  \n",
    "    # 'packages_path': Cobaya's protected key of the input dictionary.\n",
    "    # This is the variable you need to update\n",
    "    # if you are running Cobaya with cobaya_modules (option (2) above).\n",
    "    # If you are using the conda likelihood environment or option (1),\n",
    "    # please, keep the line below commented\n",
    "    #\n",
    "    #'packages_path': modules_path,\n",
    "    #\n",
    "    #'output': Cobaya's protected key of the input dictionary.\n",
    "    # Where are the results going to be stored, in case that the sampler produce output files? \n",
    "    # For example: chains...\n",
    "    # (UC): modify the path below within 'output' to choose a name and a directory for those files\n",
    "    'output': 'chains/my_euclid_experiment',\n",
    "    #'likelihood': Cobaya's protected key of the input dictionary.\n",
    "    # (UC): The user can select which data wants to use for the analysis.\n",
    "    # Check Cobaya's documentation to see the list of the current available data experiments\n",
    "    # In this DEMO, we load the Euclid-Likelihood as an external function, and name it 'Euclid'\n",
    "    'likelihood': {'Euclid': EuclidLikelihood},\n",
    "    #'debug': Cobaya's protected key of the input dictionary.\n",
    "    # (UC): how much information you want Cobaya to print? If debug: True, it prints every single detail\n",
    "    # that is going on internally in Cobaya\n",
    "    'debug': True,\n",
    "    #'timing': Cobaya's protected key of the input dictionary.\n",
    "    # (UC): if timing: True, Cobaya returns how much time it took it to make a computation of the posterior\n",
    "    # and how much time take each of the modules to perform their tasks\n",
    "    'timing': True,\n",
    "    #'force': Cobaya's protected key of the input dictionary.\n",
    "    # (UC): if 'force': True, Cobaya forces deleting the previous output files, if found, with the same name\n",
    "    'force': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2021-06-30 15:42:13,528 [output_mpi] Creating output folder 'chains'\n",
      " 2021-06-30 15:42:13,532 [output_mpi] Output to be read-from/written-into folder 'chains', with prefix 'my_euclid_experiment'\n",
      " 2021-06-30 15:42:13,893 [camb] Importing *auto-installed* CAMB (but defaulting to *global*).\n",
      " 2021-06-30 15:42:14,086 [camb] Initialized!\n",
      " 2021-06-30 15:42:23,575 [prior] *WARNING* No sampled parameters requested! This will fail for non-mock samplers.\n",
      " 2021-06-30 15:42:23,579 [camb] Importing *auto-installed* CAMB (but defaulting to *global*).\n",
      " 2021-06-30 15:42:23,579 [camb] Initialized!\n",
      " 2021-06-30 15:42:25,474 [evaluate] Initialized!\n",
      " 2021-06-30 15:42:25,544 [evaluate] Looking for a reference point with non-zero prior.\n",
      " 2021-06-30 15:42:25,546 [prior] Reference values or pdf's for some parameters were not provided. Sampling from the prior instead for those parameters.\n",
      " 2021-06-30 15:42:25,576 [evaluate] Reference point:\n",
      "   ns = 1.181\n",
      " 2021-06-30 15:42:25,578 [evaluate] Evaluating prior and likelihoods...\n",
      " 2021-06-30 15:42:28,899 [camb.transfers] Average evaluation time for camb.transfers: 0.989536 s  (1 evaluations)\n",
      " 2021-06-30 15:42:28,900 [camb] Average evaluation time for camb: 0.00230733 s  (1 evaluations)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Pk_interpolator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4fa84d885307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# For the mcmc sampler, the dictionary contains only one chain under the key sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minfo_updated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data2/cobaya/cobaya/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(info)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mupdated_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Run the sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/sampler.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_release_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/samplers/evaluate/evaluate.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 [\"%s = %g\" % pv for pv in reference_point.items()]))\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating prior and likelihoods...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             self.one_point.add(\n\u001b[1;32m     66\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderived\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/model.py\u001b[0m in \u001b[0;36mlogposterior\u001b[0;34m(self, params_values, return_derived, make_finite, cached)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogpriors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             like = self.loglikes(params_values, return_derived=return_derived,\n\u001b[0;32m--> 408\u001b[0;31m                                  make_finite=make_finite, cached=cached, _no_check=True)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mloglikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlike\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_derived\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mlogpost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/model.py\u001b[0m in \u001b[0;36mloglikes\u001b[0;34m(self, params_values, return_derived, make_finite, cached, _no_check)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         result = self.logps(input_params, return_derived=return_derived,\n\u001b[0;32m--> 314\u001b[0;31m                             cached=cached, make_finite=make_finite)\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_derived\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mloglikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/model.py\u001b[0m in \u001b[0;36mlogps\u001b[0;34m(self, input_params, return_derived, cached, make_finite)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mwant_derived\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_derived\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mdependency_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepend_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                 cached=cached, **params)\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mloglikes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/theory.py\u001b[0m in \u001b[0;36mcheck_cache_and_compute\u001b[0;34m(self, dependency_params, want_derived, cached, **params_values_dict)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_derived\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams_values_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0malways_stop_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/cobaya/cobaya/likelihood.py\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self, state, want_derived, **params_values_dict)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mderived\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwant_derived\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m  \u001b[0;31m# in case of exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derived\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mderived\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams_values_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computed log-likelihood = %g\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwant_derived\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/cobaya_interface.py\u001b[0m in \u001b[0;36mlogp\u001b[0;34m(self, **params_values)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cosmo_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosmo_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z_win'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         loglike = self.likefinal.loglike(self.cosmo.cosmo_dic,\n\u001b[0;32m--> 341\u001b[0;31m                                          self.fiducial_cosmology.cosmo_dic)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloglike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/like_calc/euclike.py\u001b[0m in \u001b[0;36mloglike\u001b[0;34m(self, dictionary, dictionary_fiducial)\u001b[0m\n\u001b[1;32m    377\u001b[0m                                     dmt, self.specinvcovfinal), dmt)\n\u001b[1;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_photo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphotothvec\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_photo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;31m# Only addition below if no cross-covariance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_photo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/like_calc/euclike.py\u001b[0m in \u001b[0;36mloglike_photo\u001b[0;34m(self, dictionary, full_photo)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 self.data_ins.nz_dict_GC_Phot)\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Obtain the theory vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mtheoryvec_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_photo_theory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphot_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_photo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mloglike_photo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfull_photo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/like_calc/euclike.py\u001b[0m in \u001b[0;36mcreate_photo_theory\u001b[0;34m(self, phot_ins, full_photo)\u001b[0m\n\u001b[1;32m    170\u001b[0m                                          \u001b[0;32mfor\u001b[0m \u001b[0mell\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                                          \u001b[0;34m[\u001b[0m\u001b[0;34m'ells'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                                          \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/like_calc/euclike.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                          \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                          self.indices_diagonal_wl])\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Obtain the theory for XC-Phot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfull_photo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/EUCLID/likelihood-implementation/likelihood/photometric_survey/photo.py\u001b[0m in \u001b[0;36mCl_WL\u001b[0;34m(self, ell, bin_i, bin_j, int_step)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mc_int_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mP_dd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pk_interpolator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mP_ii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pii'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mP_di\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pdeltai'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Pk_interpolator'"
     ]
    }
   ],
   "source": [
    "# Import Cobaya run function\n",
    "from cobaya.run import run\n",
    "\n",
    "# Let's run Cobaya\n",
    "# the function run returns\n",
    "# info_updated: an information dictionary updated with the defaults, \n",
    "# equivalent to the updated yaml file produced by the shell invocation\n",
    "# samples: a sampler object, with a sampler.products() \n",
    "# being a dictionary of results. \n",
    "# For the mcmc sampler, the dictionary contains only one chain under the key sample.\n",
    "\n",
    "info_updated, samples = run(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the printed output by Cobaya**: \n",
    "- output: it tells you details about the output \n",
    "- Prior: it shows the values sampled by the prior. In this case, our parameters of interest are all fixed so Cobaya reminds that.\n",
    "- CAMB: it calls the theory code you wanted to use (CAMB/CLASS, in this case, CAMB, and where it is installed).\n",
    "\n",
    "You see these outputs repeated twice because the CLOE calls `Cobaya` internally twice, within the EuclidLikelihood, to calculate the fiducial cosmology.\n",
    "- evaluate: the evaluate sampler gets initialized, looks for a point and evaluates the posterior. \n",
    "\n",
    "Note that since the option timing is True in the info dictionary, Cobaya tells you how much time it took to compute the likelihood (euclid) and CAMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Run `Cobaya model` with Euclid-Likelihood as external likelihood \n",
    "\n",
    "**User-case**: the analysis tool `Cobaya` has a wrapper of its `evaluate` sampler (the one executed in CASE 1), called `model`, that allows to:\n",
    "\n",
    "* make an evaluation in a single point of prior, likelihood and posterior distributions\n",
    "* retrieve derived paramaters and other quantities\n",
    "\n",
    "The `model` wrapper is useful for debbuging as well (i.e: imagine you are running a MCMC sampling to find the best-fit values of a given model, and the MCMC chains get stuck for unknown reasons). With the `model`, you can investigate what is going on internally in `Cobaya` at each step of the sampling algorithm.\n",
    "\n",
    "Moreover, our EuclidLikelihood code, which is designed to work as an external likelihood code for `Cobaya` relies on this `model` wrapper to make the connection between `Cobaya`, the likelihood calculation and the computation of the theoretical predictions (see **CASE 2.1** for further information).\n",
    "\n",
    "In this **CASE 2**, we will see how to activate the `model` wrapper, how to use it and how to understand the output it provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First: import model wrapper of Cobaya \n",
    "from cobaya.model import get_model\n",
    "\n",
    "# The `get_model` function of Cobaya imported in the line above needs a yaml or dictionary as an argument\n",
    "# exacltly the same as the function `run` in cell 9 also needs.\n",
    "#\n",
    "# We measure the time to give us an estimation of how much time it takes to make the inizialitation of the \n",
    "# likelihood\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Second: create an instance of the `model` wrapper called model\n",
    "model = get_model(info)\n",
    "print('Time for initialization of the likelihood: ', time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the printed output by Cobaya**: \n",
    "\n",
    "- Model: it tells you Cobaya is using the `model` wrapper and it's reading the info dictionary.\n",
    "- Prior: it shows the values sampled by the prior. In this case, our parameters of interest are all fixed so Cobaya reminds that.\n",
    "- CAMB: it calls the theory code you wanted to use (CAMB/CLASS, in this case, CAMB, and where it is installed).\n",
    "\n",
    "You see these outputs repeated twice because the CLOE calls Cobaya internally twice, within the EuclidLikelihood, to calculate the fiducial cosmology.\n",
    "\n",
    "It takes around 10 seconds to initialize the likelihood (reading OU-LE3 data and computing fiducial cosmology) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalities of the `model` wrapper:\n",
    "As mentioned above, we can get an insight of what `Cobaya` is doing using the `model` object. We can ask, for instance:\n",
    " * (1) which quantities were **required** by the likelihood code and asked to the Boltzman solver throught `Cobaya`\n",
    " * (2) at which values (i.e: redshift, scales...) those requirements were **requested**.\n",
    " \n",
    "To see how to ask for these quantities, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (1) Requirements needed by the likelihood code.\n",
    "# That means, which quantities are we asking to the Boltzman (CAMB/CLASS) through Cobaya?\n",
    "print('\\n Requirements \\n')\n",
    "print(model.provider.requirement_providers)\n",
    "# (2) At which values have the requirements been requested (redshift, scales...)?\n",
    "print('\\n Requested \\n')\n",
    "print(model.requested())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.provider.requirement_providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3) With the `model` wrapper you can also make an evaluation of the prior, likelihood and posterior distributions\n",
    "\n",
    "ATTENTION: we initialized the `model` wrapper by reading the `info` dictionary above. This `info` dictionary has almost all the parameters of interest fixed (the only sampled parameters is $n_s$). Therefore, to make an evaluation of the probability distributions (priors, likelihoods and posterior), you need:\n",
    "\n",
    "* First: get a point of the sampled parameters from the prior distribution \n",
    "* Second: pass this point to the logposterior method\n",
    "\n",
    "See comments in the cell below to understand how to retrieve the values of these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# At the moment, we are sampling only ns\n",
    "# if there are sampled parameters, we need first to obtain a value from the prior\n",
    "# i.e: (FIRST)\n",
    "point = dict(zip(model.parameterization.sampled_params(),\n",
    "                 model.prior.sample(ignore_external=True)[0]))\n",
    "t1 = time.time()\n",
    "# (3) Make a computation of the logposterior on that point\n",
    "logposterior = model.logposterior(point)\n",
    "# If there were no sampled parameters, you can simply do\n",
    "#logposterior = model.logposterior({})\n",
    "t2 = time.time()\n",
    "\n",
    "# Note that we are measuring the time for illustration purposes only.\n",
    "\n",
    "print('Time to compute the logposterior: ', t2-t1)\n",
    "print('Full log-posterior:')\n",
    "print('   logposterior: %g' % logposterior.logpost)\n",
    "print('   logpriors: %r' % dict(zip(list(model.prior), logposterior.logpriors)))\n",
    "print('   loglikelihoods: %r' % dict(zip(list(model.likelihood), logposterior.loglikes)))\n",
    "print('   derived params: %r' % dict(zip(list(model.parameterization.derived_params()), logposterior.derived)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2.1: EuclidLikelihood package\n",
    "\n",
    "**User-case**: this case allows the user to go one level deeper down into the CLOE code, so that the user can retrieve and plot:\n",
    "\n",
    "* Benchmark data used as mock data during the calculation of the likelihood. For instance:\n",
    "    * $n(z)$: galaxy density distributions\n",
    "* Cosmological quantities provided by the Boltzman solver through `Cobaya`. For example:\n",
    "    * $H(z)$: Huble factor\n",
    "    * $r(z)$, $D_A$: comoving and angular diameter distances\n",
    "    * $\\sigma_8$, $f\\sigma_8$: amplitude of the (linear) power spectrum on the scale of 8 $h^{-1} Mpc$ and the product of the growth rate times the amplitude of the power spectrum at that same scale.\n",
    "* Internal cosmological quantities computed by the EuclidLikelihood package itself:\n",
    "    * $D(z)$, $f(z)$: growth factor and growth rate\n",
    "    * $P_m, P_{gg}, P_{g\\delta}, P_{\\delta i}, P_{gi}, P_{ii}$: matter power spectrum obtained by the Boltzman solver by `Cobaya` and different power spectra\n",
    "* Theoretical predictions of the photometric and spectroscopic observables\n",
    "    * $W_i^{GC}, W_i^{\\gamma}, W_i^{IA}$: window functions or kernel for Galaxy Clustering (GC), Shear ($\\gamma$) and Intrinsic Aligment.\n",
    "    * $C_\\ell$: angular power spectra\n",
    "    * $P_\\ell$: Legendre multiples\n",
    "* computation of the $\\chi^2$\n",
    "\n",
    "To be able to access all these quantities and get a grasp of what EuclidLikelihood actually does, **you need to have loaded an instance of the `model` wrapper of Cobaya (explained in CASE 2)**. In reality, what we are doing in this **CASE 2.1** is to reproduce the steps that are done internally by `Cobaya` at each step of the sampling procedure within the file `cobaya_interface.py` of the likelihood package. Therefore, understanding **CASE 2.1** will also help the user to understand the details of the EuclidLikelihood source code and structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the class EuclidLikelihood\n",
    "like = EuclidLikelihood()\n",
    "\n",
    "# Initialize default parameters for redshift, k-array, fiducial cosmology...\n",
    "like.initialize()\n",
    "\n",
    "# Get the cosmo_dictionary where all the cosmology + theory parameters are saved\n",
    "# ATTENTION: as explained above, you need to pass the `cobaya wrapper model` initialized \n",
    "# in CASE 2 as an argument of the function, as well as the parameters of your theory.\n",
    "# In CASE 1, when only Cobaya run is used, it creates internally this `model` instance itself\n",
    "like.passing_requirements(model, **model.provider.params)\n",
    "\n",
    "# Update the cosmology dictionary with interpolators + basic quantities such as\n",
    "# P_gg, P_delta...\n",
    "like.cosmo.update_cosmo_dic(like.cosmo.cosmo_dic['z_win'], 0.05)\n",
    "\n",
    "# Show what the cosmo_dic actually contains\n",
    "print('\\nKeys of the cosmo_dic: \\n', list(like.cosmo.cosmo_dic.keys()))\n",
    "print('\\nKeys of the nuisance params within cosmo_dic: \\n', list(like.cosmo.cosmo_dic['nuisance_parameters'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also access the quantities of the euclike module as follows:\n",
    "# This function will return the loglike\n",
    "loglike = like.likefinal.loglike(like.cosmo.cosmo_dic, like.fiducial_cosmology.cosmo_dic)\n",
    "# This loglike agrees with the one read by Cobaya if the Likelihood external code works fine\n",
    "print('loglike: ', loglike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the computation of the chi2, you can access the following quantities of the euclike module\n",
    "print('\\nList of attributes of the euclike object: \\n', list(vars(like.likefinal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print, for instance, the mean of the redshift bins used in the computation\n",
    "print(like.likefinal.zkeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot internal quantities and cosmological observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the background quantities of cosmo_dic\n",
    "# See http://pf-ist-likelihood.pages.euclid-sgs.uk/likelihood-implementation/likelihood.cosmo.cosmology.html\n",
    "# for extra information\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18,10))\n",
    "fig.suptitle('Background quantities')\n",
    "axs[0, 0].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['H_z_func'](like.cosmo.cosmo_dic['z_win']))\n",
    "axs[0, 0].set_xlabel(r'$z$')\n",
    "axs[0, 0].set_ylabel(r'$H(z)$ $\\left[ \\frac{km}{s\\times Mpc}\\right]$')\n",
    "axs[0, 1].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['d_z_func'](like.cosmo.cosmo_dic['z_win']),\n",
    "              label = r'$D_A(z)$ $[Mpc]$')\n",
    "axs[0, 1].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['r_z_func'](like.cosmo.cosmo_dic['z_win']), '--',\n",
    "              label = r'$r(z)$')\n",
    "axs[0, 1].set_xlabel(r'$z$')\n",
    "axs[0, 1].legend(frameon=False)\n",
    "axs[1, 0].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['sigma8_z_func'](like.cosmo.cosmo_dic['z_win']),\n",
    "              label = r'$\\sigma_8(z)$')\n",
    "axs[1, 0].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['fsigma8_z_func'](like.cosmo.cosmo_dic['z_win']),\n",
    "              '--', label = r'$f\\sigma_8(z)$')\n",
    "axs[1, 0].set_xlabel(r'$z$')\n",
    "axs[1, 0].legend(frameon=False)\n",
    "axs[1, 1].plot(like.cosmo.cosmo_dic['z_win'], like.cosmo.cosmo_dic['D_z_k'],\n",
    "              label = r'$D(z)$')\n",
    "axs[1, 1].plot(like.cosmo.cosmo_dic['z_win'], \n",
    "               like.cosmo.cosmo_dic['f_z'](like.cosmo.cosmo_dic['z_win']),\n",
    "              '--', label = r'$f = f\\sigma_8/\\sigma_8$')\n",
    "axs[1, 1].set_xlabel(r'$z$');\n",
    "axs[1, 1].legend(frameon=False)\n",
    "plt.subplots_adjust(top=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's plot the matter power spectrum and other spectra quantities\n",
    "# 'Pgg_spec', 'Pgg_phot', 'Pgdelta_phot', 'Pgdelta_spec', 'Pii', 'Pdeltai', 'Pgi_phot', 'Pgi_spec' \n",
    "# See http://pf-ist-likelihood.pages.euclid-sgs.uk/likelihood-implementation/likelihood.cosmo.cosmology.html\n",
    "# for extra information\n",
    "\n",
    "# WARNING: AT THE MOMENT, THE LARGE SCALES (small k values) in the power spectra are not physical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fig, axs = plt.subplots(9, 2, figsize=(20,47))\n",
    "fig.suptitle('Spectra')\n",
    "ks=np.logspace(-5, 0, 100)\n",
    "axs[0, 0].loglog(ks, like.cosmo.cosmo_dic['Pk_delta'].P(like.cosmo.cosmo_dic['z_win'][0], ks), 'o', \n",
    "           label=r\"z = {:.2f}\".format(like.cosmo.cosmo_dic['z_win'][0]))\n",
    "axs[0, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[0, 0].set_ylabel(r'$P_m [Mpc^{-3}]$')\n",
    "axs[0, 0].legend(frameon=False)\n",
    "#------\n",
    "axs[0, 1].loglog(ks, like.cosmo.cosmo_dic['Pk_delta'].P(like.cosmo.cosmo_dic['z_win'][-1], ks), 'o', \n",
    "           label=r\"z = {:.2f}\".format(like.cosmo.cosmo_dic['z_win'][-1]))\n",
    "axs[0, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[0, 1].set_ylabel(r'$P_m [Mpc^{-3}]$')\n",
    "axs[0, 1].legend(frameon=False)\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[1, 0].loglog(k, like.cosmo.cosmo_dic['Pgg_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][25], k, 1), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][25]) if i == 0 else \"\")\n",
    "    axs[1, 1].loglog(k, like.cosmo.cosmo_dic['Pgg_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][35], k, 1), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][35]) if i == 0 else \"\")\n",
    "axs[1, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[1, 0].set_ylabel(r'$P_{gg}^{spec} [Mpc^{-3}]$')\n",
    "axs[1, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[1, 1].set_ylabel(r'$P_{gg}^{spec} [Mpc^{-3}]$')\n",
    "axs[1, 0].legend(frameon=False)\n",
    "axs[1, 1].legend(frameon=False)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[2, 0].loglog(k, like.cosmo.cosmo_dic['Pgg_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[2, 1].loglog(k, like.cosmo.cosmo_dic['Pgg_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[2, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[2, 0].set_ylabel(r'$P_{gg}^{phot} [Mpc^{-3}]$')\n",
    "axs[2, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[2, 1].set_ylabel(r'$P_{gg}^{phot} [Mpc^{-3}]$')\n",
    "axs[2, 0].legend(frameon=False)\n",
    "axs[2, 1].legend(frameon=False)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[3, 0].loglog(k, like.cosmo.cosmo_dic['Pgdelta_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][25], k, 1), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][25]) if i == 0 else \"\")\n",
    "    axs[3, 1].loglog(k, like.cosmo.cosmo_dic['Pgdelta_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][35], k, 1), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][35]) if i == 0 else \"\")\n",
    "axs[3, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[3, 0].set_ylabel(r'$P_{g\\delta}^{spec} [Mpc^{-3}]$')\n",
    "axs[3, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[3, 1].set_ylabel(r'$P_{g\\delta}^{spec} [Mpc^{-3}]$')\n",
    "axs[3, 0].legend(frameon=False)\n",
    "axs[3, 1].legend(frameon=False)\n",
    "\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[4, 0].loglog(k, like.cosmo.cosmo_dic['Pgdelta_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[4, 1].loglog(k, like.cosmo.cosmo_dic['Pgdelta_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[4, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[4, 0].set_ylabel(r'$P_{g\\delta}^{phot} [Mpc^{-3}])$')\n",
    "axs[4, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[4, 1].set_ylabel(r'$P_{g\\delta}^{phot} [Mpc^{-3}]$')\n",
    "axs[4, 0].legend(frameon=False)\n",
    "axs[4, 1].legend(frameon=False)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[5, 0].loglog(k, like.cosmo.cosmo_dic['Pii'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'ro', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[5, 1].loglog(k, like.cosmo.cosmo_dic['Pii'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'ro', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[5, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[5, 0].set_ylabel(r'$P_{ii} [Mpc^{-3}]$')\n",
    "axs[5, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[5, 1].set_ylabel(r'$P_{ii} [Mpc^{-3}]$')\n",
    "axs[5, 0].legend(frameon=False)\n",
    "axs[5, 1].legend(frameon=False)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[6, 0].plot(k, like.cosmo.cosmo_dic['Pdeltai'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'bo', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[6, 1].plot(k, like.cosmo.cosmo_dic['Pdeltai'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'bo', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[6, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[6, 0].set_ylabel(r'$P_{\\delta i} [Mpc^{-3}]$')\n",
    "axs[6, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[6, 1].set_ylabel(r'$P_{\\delta i} [Mpc^{-3}]$')\n",
    "axs[6, 0].legend(frameon=False, loc=4)\n",
    "axs[6, 1].legend(frameon=False, loc=4)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[7, 0].plot(k, like.cosmo.cosmo_dic['Pgi_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[7, 1].plot(k, like.cosmo.cosmo_dic['Pgi_phot'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'go', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[7, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[7, 0].set_ylabel(r'$P_{gi}^{phot} [Mpc^{-3}]$')\n",
    "axs[7, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[7, 1].set_ylabel(r'$P_{gi}^{phot} [Mpc^{-3}]$')\n",
    "axs[7, 0].legend(frameon=False, loc=4)\n",
    "axs[7, 1].legend(frameon=False, loc=4)\n",
    "\n",
    "#------\n",
    "for i, k in enumerate(ks):\n",
    "    axs[8, 0].plot(k, like.cosmo.cosmo_dic['Pgi_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][0], k), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][0]) if i == 0 else \"\")\n",
    "    axs[8, 1].plot(k, like.cosmo.cosmo_dic['Pgi_spec'](\n",
    "    like.cosmo.cosmo_dic['z_win'][-1], k), 'ko', label=r\"z = {:.2f}\".format(\n",
    "        like.cosmo.cosmo_dic['z_win'][-1]) if i == 0 else \"\")\n",
    "axs[8, 0].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[8, 0].set_ylabel(r'$P_{gi}^{spec} [Mpc^{-3}]$')\n",
    "axs[8, 1].set_xlabel(r'$k$ $[Mpc^{-1}]$')\n",
    "axs[8, 1].set_ylabel(r'$P_{gi}^{spec} [Mpc^{-3}]$')\n",
    "axs[8, 0].legend(frameon=False, loc=4)\n",
    "axs[8, 1].legend(frameon=False, loc=4)\n",
    "\n",
    "plt.subplots_adjust(top=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the initial data read by the like_calc module (class Euclike). \n",
    "# This data is at the moment the fiducial data store within the data folder of the repository\n",
    "\n",
    "# i.e: let's plot the galaxy distributions for GC-photo(z) and WL (AT THE MOMENT, THEY ARE THE SAME)\n",
    "zs = np.linspace(0, 4, 1000)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(18,17))\n",
    "for key, value in like.likefinal.data_ins.nz_dict_GC_Phot.items():\n",
    "    axs[0].plot(zs, value(zs), label = key)\n",
    "for key, value in like.likefinal.data_ins.nz_dict_WL.items():\n",
    "    axs[1].plot(zs, value(zs), label = key)\n",
    "axs[0].set_xlabel(r'$z$')\n",
    "axs[0].set_ylabel(r'$n(z)$ $[sr^{-1}]$')\n",
    "axs[0].set_title(r'$GC-photo(z)$')\n",
    "axs[0].legend(frameon=False, ncol=2)\n",
    "axs[1].set_xlabel(r'$z$')\n",
    "axs[1].set_ylabel(r'$n(z)$ $[sr^{-1}]$')\n",
    "axs[1].set_title(r'$WL$')\n",
    "axs[1].legend(frameon=False, ncol=2);\n",
    "#plt.subplots_adjust(top=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the window functions for the photometric observables\n",
    "# For that, you need to import the photo class and read the cosmology dictionary and the n(z) distributions above\n",
    "from likelihood.photometric_survey.photo import Photo\n",
    "photo = Photo(like.cosmo.cosmo_dic, like.likefinal.data_ins.nz_dict_WL, like.likefinal.data_ins.nz_dict_GC_Phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "color = list(mcolors.TABLEAU_COLORS.values())\n",
    "fig, axs = plt.subplots(3, 1, figsize=(18,12))\n",
    "for i in range(0, 10):\n",
    "    print('bin: {}'.format(i+1))\n",
    "    axs[0].plot(zs, photo.GC_window(zs, i+1), '-', label='n{}'.format(i+1))\n",
    "    axs[1].plot(photo.z_winterp, photo.WL_window(i+1), '.', color = color[i], label='n{}'.format(i+1) if i == 0 else \"\")\n",
    "    axs[2].plot(zs, photo.IA_window(zs, i+1), label='n{}'.format(i+1))\n",
    "axs[0].set_xlabel(r'$z$')\n",
    "axs[0].set_ylabel(r'$W_i^{GC-photo}$ $[Mpc^{-1}]$')\n",
    "axs[0].legend(frameon=False, ncol=2)\n",
    "axs[1].set_xlabel(r'$z$')\n",
    "axs[1].set_ylabel(r'$W_i^{\\gamma}$ $[Mpc^{-1}]$')\n",
    "axs[1].legend(frameon=False, ncol=2);\n",
    "axs[2].set_xlabel(r'$z$')\n",
    "axs[2].set_ylabel(r'$W_i^{IA}$ $[Mpc^{-1}]$')\n",
    "axs[2].legend(frameon=False, ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Euclid final observables\n",
    "\n",
    "We can use the auxiliary module of the CLOE to plot the final observables. **Have in mind that the predicted values for the angular power spectra will probably not agree with the fiducial plotted values from the Benchmark data as we are currently sampling $n_s$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the Plotter Class\n",
    "from likelihood.auxiliary.plotter import Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model.parameterization.sampled_params():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak-Lensing angular power spectrum according to the IST:L recipe \n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "for param in model.parameterization.sampled_params().keys():\n",
    "        pl_label = '{}={:.4f}'.format(param, model.parameterization.sampled_params()[param])\n",
    "        print('WARNING: param {} is being sampled! Possible missmatch between benchmark and theoretical prediction'.format(param))\n",
    "pl_inst = Plotter(cosmo_dic=like.cosmo.cosmo_dic)\n",
    "ax1 = pl_inst.plot_external_Cl_phot(1, 1, ax1, probe='WL', pl_label='Benchmark')\n",
    "ax1 = pl_inst.plot_Cl_phot(np.logspace(1, 3.6, 10), 1, 1, ax1, probe='WL', pl_colour='r', pl_linestyle='--', pl_label=pl_label)\n",
    "ax1.set_xlabel(r'$\\ell$', fontsize=20)\n",
    "ax1.set_ylabel(r'$C_\\ell$ $[sr^{-1}]$', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('Weak-Lensing bin1');\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC-photo(z) angular power spectrum\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "for param in model.parameterization.sampled_params().keys():\n",
    "        pl_label = '{}={:.4f}'.format(param, model.parameterization.sampled_params()[param])\n",
    "        print('WARNING: param {} is being sampled! Possible missmatch between benchmark and theoretical prediction'.format(param))\n",
    "ax1 = pl_inst.plot_external_Cl_phot(1, 1, ax1, probe='GC-Phot', pl_label='Benchmark')\n",
    "ax1 = pl_inst.plot_Cl_phot(np.logspace(1, 3.6, 10), 1, 1, ax1, probe='GC-Phot', pl_colour='r', pl_linestyle='--', pl_label=pl_label)\n",
    "ax1.set_xlabel(r'$\\ell$', fontsize=20)\n",
    "ax1.set_ylabel(r'$C_\\ell$ $[sr^{-1}]$', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('GC-photo(z) bin1');\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-correlation WL x GC-photo(z) angular power spectrum\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "for param in model.parameterization.sampled_params().keys():\n",
    "        pl_label = '{}={:.4f}'.format(param, model.parameterization.sampled_params()[param])\n",
    "        print('WARNING: param {} is being sampled! Possible missmatch between benchmark and theoretical prediction'.format(param))\n",
    "ax1 = pl_inst.plot_external_Cl_XC(1, 1, ax1, pl_label='Benchmark')\n",
    "ax1 = pl_inst.plot_Cl_XC(np.logspace(1, 3.6, 10), 1, 1, ax1, pl_colour='r', pl_linestyle='--', pl_label=pl_label)\n",
    "ax1.set_xlabel(r'$\\ell$', fontsize=20)\n",
    "ax1.set_ylabel(r'$C_\\ell$ $[sr^{-1}]$', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "#ax1.set_yscale('log')\n",
    "ax1.set_title('WL x GC-photo(z) bin1');\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC-spec observable\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "for param in model.parameterization.sampled_params().keys():\n",
    "        pl_label = '{}={:.4f}'.format(param, model.parameterization.sampled_params()[param])\n",
    "        print('WARNING: param {} is being sampled! Possible missmatch between benchmark and theoretical prediction'.format(param))\n",
    "ax1 = pl_inst.plot_external_GC_spec(\"1.2\", 2, ax1, pl_label='Benchmark')\n",
    "ax1 = pl_inst.plot_GC_spec_multipole(1.2, np.linspace(0.01, 0.5), 2, ax1, pl_colour='r', pl_linestyle='--', pl_label=pl_label)\n",
    "ax1.set_xlabel(r'$k$ $[Mpc^{-1}]$', fontsize=20)\n",
    "ax1.set_ylabel(r'$P_\\ell$', fontsize=20)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('GC-spec bin1');\n",
    "ax1.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
